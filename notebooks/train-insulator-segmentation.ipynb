{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras\n\nimport segmentation_models as sm\n\nfrom segmentation_models.utils import set_trainable\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers.legacy import Adam, SGD\n\nimport numpy as np \nimport os\nimport glob\nimport skimage.io as io\nimport skimage.transform as trans\nimport matplotlib.pyplot as plt\nimport cv2\nimport time\nfrom keras.callbacks import ModelCheckpoint, CSVLogger","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_data(img,mask,flag_multi_class,num_class):\n    if(flag_multi_class):\n        img = img / 255\n        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n        new_mask = np.zeros(mask.shape + (num_class,))\n        for i in range(num_class):\n            new_mask[mask == i,i] = 1\n        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n        mask = new_mask\n    elif(np.max(img) > 1):\n        img = img / 255\n        mask = mask /255\n        mask[mask >= 0.5] = 1\n        mask[mask < 0.5] = 0\n    return (img,mask)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_generator(batch_size, \n                   train_path,\n                   image_folder,\n                   mask_folder,\n                   aug_dict,\n                   image_color_mode = \"grayscale\",\n                   mask_color_mode = \"grayscale\", \n                   image_save_prefix  = \"image\", \n                   mask_save_prefix  = \"mask\",\n                   flag_multi_class = False,\n                   num_class = 1,\n                   save_to_dir = None,\n                   target_size = (width_size,height_size),\n                   seed = 1):\n    '''\n    Can generate image and mask at the same time. Use the same seed for image_datagen and mask_datagen to ensure \n    the transformation for image and mask is the same. If you want to visualize the results of generator, set \n    save_to_dir = \"your path\"\n    '''\n    \n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_directory(train_path,\n                                                        classes = [image_folder],\n                                                        class_mode = None,\n                                                        color_mode = image_color_mode,\n                                                        target_size = target_size,\n                                                        batch_size = batch_size,\n                                                        save_to_dir = save_to_dir,\n                                                        save_prefix  = image_save_prefix,\n                                                        seed = seed)\n    \n    mask_generator = mask_datagen.flow_from_directory(train_path,\n                                                        classes = [mask_folder],\n                                                        class_mode = None,\n                                                        color_mode = mask_color_mode,\n                                                        target_size = target_size,\n                                                        batch_size = batch_size,\n                                                        save_to_dir = None,\n                                                        save_prefix  = mask_save_prefix,\n                                                        seed = seed)\n    \n    train_generator = zip(image_generator, mask_generator)\n    \n    for (img,mask) in train_generator:\n        img,mask = adjust_data(img, mask, flag_multi_class, num_class)\n        yield (img,mask)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_generator(batch_size,\n                  test_path,\n                  image_folder,\n                  mask_folder,\n                  image_color_mode = \"grayscale\",\n                  mask_color_mode = \"grayscale\",\n                  image_save_prefix  = \"image\",\n                  mask_save_prefix  = \"mask\",\n                  flag_multi_class = False,\n                  num_class = 1, \n                  save_to_dir = None,\n                  target_size = (width_size,height_size), seed = 2):\n    '''\n    Can generate image and mask at the same time. Use the same seed for image_datagen and mask_datagen to ensure \n    the transformation for image and mask is the same. If you want to visualize the results of generator, set \n    save_to_dir = \"your path\"\n    '''\n    \n    image_datagen = ImageDataGenerator()\n    mask_datagen = ImageDataGenerator()\n    \n    image_generator = image_datagen.flow_from_directory(test_path,\n                                                        classes = [image_folder],\n                                                        class_mode = None,\n                                                        color_mode = image_color_mode,\n                                                        target_size = target_size,\n                                                        batch_size = batch_size,\n                                                        save_to_dir = save_to_dir,\n                                                        save_prefix  = image_save_prefix,\n                                                        seed = seed)\n    \n    mask_generator = mask_datagen.flow_from_directory(test_path,\n                                                      classes = [mask_folder],\n                                                      class_mode = None,\n                                                      color_mode = mask_color_mode,\n                                                      target_size = target_size,\n                                                      batch_size = batch_size,\n                                                      save_to_dir = save_to_dir,\n                                                      save_prefix  = mask_save_prefix,\n                                                      seed = seed)\n    \n    test_generator = zip(image_generator, mask_generator)\n    \n    for (img,mask) in test_generator:\n        img,mask = adjust_data(img,mask,flag_multi_class,num_class)\n        yield (img,mask)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_dataset = '../dataset_insulators/'\ntrain_data_path = '../dataset_insulators/train/images'\ntest_data_path = '../dataset_insulators/validation/images/'\n\nnum_of_imgs_train = sum(len(files) for _, _, files in os.walk(train_data_path))\nnum_of_imgs_test = sum(len(files) for _, _, files in os.walk(test_data_path))\n\nnum_of_train_samples = int(num_of_imgs_train)\nnum_of_validation_samples  = int(num_of_imgs_test)\nprint(\"Train Images : \", num_of_train_samples)\nprint(\"Validation Images  : \", num_of_validation_samples)\n\nbatch_size = 2\ntrain_samples = num_of_train_samples\ntest_samples = num_of_validation_samples\nsteps_p_epoch = int(train_samples / batch_size)\nval_steps = int(test_samples / batch_size)\nnr_epochs = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_Unet_resnet34 = sm.Unet(backbone_name='resnet34', encoder_weights='imagenet', encoder_freeze=True, classes=1, activation='sigmoid')\nmodel_Linknet_resnet34 = sm.Linknet(backbone_name='resnet34', encoder_weights='imagenet', encoder_freeze=True, classes=1, activation='sigmoid')\nmodel_Unet_vgg16 = sm.Unet(backbone_name='vgg16', encoder_weights='imagenet', encoder_freeze=True, classes=1, activation='sigmoid')\nmodel_Linknet_vgg16 = sm.Linknet(backbone_name='vgg16', encoder_weights='imagenet', encoder_freeze=True, classes=1, activation='sigmoid')\n\nall_models = [model_Unet_resnet34, model_Linknet_resnet34, model_Unet_vgg16, model_Linknet_vgg16]\nall_names_models = [\"model_Unet_resnet34\", \"model_Linknet_resnet34\", \"model_Unet_vgg16\", \"model_Linknet_vgg16\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index in range(len(all_names_models)):\n    since = time.time()\n    BACKBONE = all_names_models[index].split('_')[-1]\n    DECODER = all_names_models[index].split('_')[-2]\n    \n    preprocess_input = sm.get_preprocessing(BACKBONE)\n\n    trainGene = train_generator(batch_size, \n                               path_dataset, \n                               'train',\n                               'mask_train',\n                               data_gen_args,\n                               save_to_dir = None, \n                               image_color_mode = \"rgb\"\n                              )\n\n    testGene = test_generator(batch_size, \n                              path_dataset,\n                              'validation', \n                              'mask_validation', \n                              save_to_dir = None,\n                              image_color_mode = \"rgb\")\n    \n    model = all_models[index]\n    opt = Adam(learning_rate=1e-3)\n\n    model.compile(opt,\n                  loss=sm.losses.jaccard_loss,\n                  metrics=[sm.metrics.IOUScore(threshold=0.5),\n                  sm.metrics.FScore(threshold=0.5), \n                  sm.metrics.precision])\n\n    csv_logger = CSVLogger(BACKBONE + \"_\" + DECODER + \".csv\", append=True)\n    \n    model_checkpoint = ModelCheckpoint(BACKBONE + \"_\" + DECODER + \".hdf5\", \n                                       monitor='loss',\n                                       verbose=1,\n                                       save_best_only=True)\n    \n    history = model.fit(trainGene, \n                        validation_data = testGene,\n                        validation_steps = val_steps,\n                        steps_per_epoch = steps_p_epoch,\n                        epochs=nr_epochs,\n                        callbacks=[model_checkpoint, csv_logger])\n    \n    time_elapsed = time.time() - since\n    print(\n        f\"\\n Total time: {round(time_elapsed // 60, 2)} minutes and {round(time_elapsed % 60, 2)} seconds\"\n    )\n","metadata":{},"execution_count":null,"outputs":[]}]}