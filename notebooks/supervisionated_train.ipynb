{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Mb9UEHzz9yN"
   },
   "source": [
    "# **Aprendizado Supervisionado - Segmentação Semântica**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsiPI3wW0tgl"
   },
   "source": [
    "Importando as Bibliotecas Necessárias\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kF2RbfLN9HMZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation-models in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: image-classifiers==1.0.0 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from segmentation-models) (1.0.0)\n",
      "Requirement already satisfied: efficientnet==1.0.0 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from segmentation-models) (1.0.0)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from segmentation-models) (1.0.8)\n",
      "Requirement already satisfied: scikit-image in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n",
      "Requirement already satisfied: h5py in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.24.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.10.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2023.1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (23.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.25.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /home/pericles/miniconda3/envs/tf2/lib/python3.8/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U segmentation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_FRAMEWORK=tf.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 00:53:40.298084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 00:53:40.465268: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-18 00:53:41.409497: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/pericles/miniconda3/envs/tf2/lib/\n",
      "2023-02-18 00:53:41.409581: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/pericles/miniconda3/envs/tf2/lib/\n",
      "2023-02-18 00:53:41.409586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "%env SM_FRAMEWORK=tf.keras\n",
    "\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hV0tNxmYRsX8",
    "outputId": "3e4a48d9-207b-4d03-9d2e-e01497a3cbec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_FRAMEWORK=tf.keras\n"
     ]
    }
   ],
   "source": [
    "%env SM_FRAMEWORK=tf.keras\n",
    "\n",
    "from segmentation_models.utils import set_trainable\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD\n",
    "#from model import *\n",
    "#from data import *\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F_PJM2g6_r9Z"
   },
   "outputs": [],
   "source": [
    "def adjust_data(img,mask,flag_multi_class,num_class):\n",
    "    if(flag_multi_class):\n",
    "        img = img / 255\n",
    "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
    "        new_mask = np.zeros(mask.shape + (num_class,))\n",
    "        for i in range(num_class):\n",
    "            new_mask[mask == i,i] = 1\n",
    "        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
    "        mask = new_mask\n",
    "    elif(np.max(img) > 1):\n",
    "        img = img / 255\n",
    "        mask = mask /255\n",
    "        mask[mask >= 0.5] = 1\n",
    "        mask[mask < 0.5] = 0\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_size = 512\n",
    "height_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I0oDxoNg_ul5"
   },
   "outputs": [],
   "source": [
    "def train_generator(batch_size, \n",
    "                   train_path,\n",
    "                   image_folder,\n",
    "                   mask_folder,\n",
    "                   aug_dict,\n",
    "                   image_color_mode = \"grayscale\",\n",
    "                   mask_color_mode = \"grayscale\", \n",
    "                   image_save_prefix  = \"image\", \n",
    "                   mask_save_prefix  = \"mask\",\n",
    "                   flag_multi_class = False,\n",
    "                   num_class = 1,\n",
    "                   save_to_dir = None,\n",
    "                   target_size = (width_size,height_size),\n",
    "                   seed = 1):\n",
    "    '''\n",
    "    Can generate image and mask at the same time. Use the same seed for image_datagen and mask_datagen to ensure \n",
    "    the transformation for image and mask is the same. If you want to visualize the results of generator, set \n",
    "    save_to_dir = \"your path\"\n",
    "    '''\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(train_path,\n",
    "                                                        classes = [image_folder],\n",
    "                                                        class_mode = None,\n",
    "                                                        color_mode = image_color_mode,\n",
    "                                                        target_size = target_size,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        save_to_dir = save_to_dir,\n",
    "                                                        save_prefix  = image_save_prefix,\n",
    "                                                        seed = seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(train_path,\n",
    "                                                        classes = [mask_folder],\n",
    "                                                        class_mode = None,\n",
    "                                                        color_mode = mask_color_mode,\n",
    "                                                        target_size = target_size,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        save_to_dir = None,\n",
    "                                                        save_prefix  = mask_save_prefix,\n",
    "                                                        seed = seed)\n",
    "    \n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = adjust_data(img, mask, flag_multi_class, num_class)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "S_7JdzNO_xcl"
   },
   "outputs": [],
   "source": [
    "def test_generator(batch_size,\n",
    "                  test_path,\n",
    "                  image_folder,\n",
    "                  mask_folder,\n",
    "                  image_color_mode = \"grayscale\",\n",
    "                  mask_color_mode = \"grayscale\",\n",
    "                  image_save_prefix  = \"image\",\n",
    "                  mask_save_prefix  = \"mask\",\n",
    "                  flag_multi_class = False,\n",
    "                  num_class = 1, \n",
    "                  save_to_dir = None,\n",
    "                  target_size = (width_size,height_size), seed = 2):\n",
    "    '''\n",
    "    Can generate image and mask at the same time. Use the same seed for image_datagen and mask_datagen to ensure \n",
    "    the transformation for image and mask is the same. If you want to visualize the results of generator, set \n",
    "    save_to_dir = \"your path\"\n",
    "    '''\n",
    "    \n",
    "    image_datagen = ImageDataGenerator()\n",
    "    mask_datagen = ImageDataGenerator()\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(test_path,\n",
    "                                                        classes = [image_folder],\n",
    "                                                        class_mode = None,\n",
    "                                                        color_mode = image_color_mode,\n",
    "                                                        target_size = target_size,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        save_to_dir = save_to_dir,\n",
    "                                                        save_prefix  = image_save_prefix,\n",
    "                                                        seed = seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(test_path,\n",
    "                                                      classes = [mask_folder],\n",
    "                                                      class_mode = None,\n",
    "                                                      color_mode = mask_color_mode,\n",
    "                                                      target_size = target_size,\n",
    "                                                      batch_size = batch_size,\n",
    "                                                      save_to_dir = save_to_dir,\n",
    "                                                      save_prefix  = mask_save_prefix,\n",
    "                                                      seed = seed)\n",
    "    \n",
    "    test_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img,mask) in test_generator:\n",
    "        img,mask = adjust_data(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yliG-744F5dI"
   },
   "source": [
    "Definindo como o Modelo fará o Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e0I9SPHw_2fg"
   },
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range=180,\n",
    "                    width_shift_range=0.07,\n",
    "                    height_shift_range=0.07,\n",
    "                    shear_range=0.07,\n",
    "                    zoom_range=0.07,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Im1_50BZ1wVg"
   },
   "source": [
    "Definindo o caminho das pastas de validação e treino. Inicializando o dimensionamento das imagens que serão lidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7Iec-dZ_5fw",
    "outputId": "81c6047a-0f1b-4c36-e5a0-276b4455201a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images :  34830\n",
      "Validation Images  :  8932\n"
     ]
    }
   ],
   "source": [
    "path_dataset = '../dataset_insulators/'\n",
    "train_data_path = '../dataset_insulators/train/images'\n",
    "test_data_path = '../dataset_insulators/validation/images/'\n",
    "\n",
    "num_of_imgs_train = sum(len(files) for _, _, files in os.walk(train_data_path))\n",
    "num_of_imgs_test = sum(len(files) for _, _, files in os.walk(test_data_path))\n",
    "\n",
    "num_of_train_samples = int(num_of_imgs_train)\n",
    "num_of_validation_samples  = int(num_of_imgs_test)\n",
    "print(\"Train Images : \", num_of_train_samples)\n",
    "print(\"Validation Images  : \", num_of_validation_samples)\n",
    "\n",
    "batch_size = 2\n",
    "train_samples = num_of_train_samples\n",
    "test_samples = num_of_validation_samples\n",
    "steps_p_epoch = int(train_samples / batch_size)\n",
    "val_steps = int(test_samples / batch_size)\n",
    "nr_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXaD1XmCGomW"
   },
   "source": [
    "# Colocando a rede para treinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 00:53:57.410205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 00:53:57.467009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 00:53:57.467187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 00:54:06.779228: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 00:54:06.781708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 00:54:06.781995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 00:54:06.782116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 00:54:07.460613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 00:54:07.460819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 00:54:07.460933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 00:54:07.461027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2107 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#3227\n",
    "model_Unet_resnet34 = sm.Unet(backbone_name='resnet34', encoder_weights='imagenet', encoder_freeze=True, classes=1, activation='sigmoid')\n",
    "\n",
    "#1886\n",
    "model_Linknet_resnet34 = sm.Linknet(backbone_name='resnet34', encoder_weights='imagenet', encoder_freeze=True, classes=1, activation='sigmoid')\n",
    "\n",
    "#2921.6\n",
    "model_Unet_vgg16 = sm.Unet(backbone_name='vgg16', encoder_weights='imagenet', encoder_freeze=True, classes=1, activation='sigmoid')\n",
    "\n",
    "#2897.14 Segundos medios por epoca\n",
    "model_Linknet_vgg16 = sm.Linknet(backbone_name='vgg16', encoder_weights='imagenet', encoder_freeze=True, classes=1, activation='sigmoid')\n",
    "\n",
    "all_models = [model_Unet_resnet34, model_Linknet_resnet34, model_Unet_vgg16, model_Linknet_vgg16]\n",
    "all_names_models = [\"model_Unet_resnet34\", \"model_Linknet_resnet34\", \"model_Unet_vgg16\", \"model_Linknet_vgg16\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "W4sZ8JgAYPgv",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34830 images belonging to 1 classes.\n",
      "Found 34830 images belonging to 1 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 00:54:40.428574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-02-18 00:54:41.489534: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-18 00:54:41.490487: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-18 00:54:41.490520: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:85] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-02-18 00:54:41.491099: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-18 00:54:41.491162: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-02-18 00:54:42.698903: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:42.698931: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:43.109611: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:43.109643: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:43.528456: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:43.528484: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:44.150047: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.16GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:44.150085: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.16GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:45.057355: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-02-18 00:54:45.057384: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17415/17415 [==============================] - ETA: 0s - loss: 0.1010 - iou_score: 0.9001 - f1-score: 0.9432 - precision: 0.9430Found 8932 images belonging to 1 classes.\n",
      "Found 8932 images belonging to 1 classes.\n",
      "\n",
      "Epoch 1: loss improved from inf to 0.10097, saving model to resnet34_Unet.hdf5\n",
      "17415/17415 [==============================] - 3313s 189ms/step - loss: 0.1010 - iou_score: 0.9001 - f1-score: 0.9432 - precision: 0.9430 - val_loss: 0.0540 - val_iou_score: 0.9463 - val_f1-score: 0.9716 - val_precision: 0.9858\n",
      "Epoch 2/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0540 - iou_score: 0.9462 - f1-score: 0.9713 - precision: 0.9723\n",
      "Epoch 2: loss improved from 0.10097 to 0.05396, saving model to resnet34_Unet.hdf5\n",
      "17415/17415 [==============================] - 3205s 184ms/step - loss: 0.0540 - iou_score: 0.9462 - f1-score: 0.9713 - precision: 0.9723 - val_loss: 0.0514 - val_iou_score: 0.9488 - val_f1-score: 0.9728 - val_precision: 0.9855\n",
      "Epoch 3/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0462 - iou_score: 0.9539 - f1-score: 0.9758 - precision: 0.9762\n",
      "Epoch 3: loss improved from 0.05396 to 0.04617, saving model to resnet34_Unet.hdf5\n",
      "17415/17415 [==============================] - 3205s 184ms/step - loss: 0.0462 - iou_score: 0.9539 - f1-score: 0.9758 - precision: 0.9762 - val_loss: 0.0579 - val_iou_score: 0.9422 - val_f1-score: 0.9665 - val_precision: 0.9864\n",
      "Epoch 4/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0421 - iou_score: 0.9580 - f1-score: 0.9781 - precision: 0.9783\n",
      "Epoch 4: loss improved from 0.04617 to 0.04209, saving model to resnet34_Unet.hdf5\n",
      "17415/17415 [==============================] - 3206s 184ms/step - loss: 0.0421 - iou_score: 0.9580 - f1-score: 0.9781 - precision: 0.9783 - val_loss: 0.0590 - val_iou_score: 0.9411 - val_f1-score: 0.9670 - val_precision: 0.9926\n",
      "Epoch 5/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0406 - iou_score: 0.9595 - f1-score: 0.9788 - precision: 0.9789\n",
      "Epoch 5: loss improved from 0.04209 to 0.04059, saving model to resnet34_Unet.hdf5\n",
      "17415/17415 [==============================] - 3206s 184ms/step - loss: 0.0406 - iou_score: 0.9595 - f1-score: 0.9788 - precision: 0.9789 - val_loss: 0.0496 - val_iou_score: 0.9504 - val_f1-score: 0.9729 - val_precision: 0.9912\n",
      "\n",
      " Total time: 268.0 minutes and 56.7 seconds\n",
      "Found 34830 images belonging to 1 classes.\n",
      "Found 34830 images belonging to 1 classes.\n",
      "Epoch 1/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.1166 - iou_score: 0.8847 - f1-score: 0.9342 - precision: 0.9336Found 8932 images belonging to 1 classes.\n",
      "Found 8932 images belonging to 1 classes.\n",
      "\n",
      "Epoch 1: loss improved from inf to 0.11662, saving model to resnet34_Linknet.hdf5\n",
      "17415/17415 [==============================] - 2185s 125ms/step - loss: 0.1166 - iou_score: 0.8847 - f1-score: 0.9342 - precision: 0.9336 - val_loss: 0.0645 - val_iou_score: 0.9359 - val_f1-score: 0.9659 - val_precision: 0.9698\n",
      "Epoch 2/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0700 - iou_score: 0.9303 - f1-score: 0.9625 - precision: 0.9630\n",
      "Epoch 2: loss improved from 0.11662 to 0.07003, saving model to resnet34_Linknet.hdf5\n",
      "17415/17415 [==============================] - 2183s 125ms/step - loss: 0.0700 - iou_score: 0.9303 - f1-score: 0.9625 - precision: 0.9630 - val_loss: 0.0595 - val_iou_score: 0.9408 - val_f1-score: 0.9684 - val_precision: 0.9807\n",
      "Epoch 3/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0610 - iou_score: 0.9392 - f1-score: 0.9676 - precision: 0.9678\n",
      "Epoch 3: loss improved from 0.07003 to 0.06101, saving model to resnet34_Linknet.hdf5\n",
      "17415/17415 [==============================] - 2183s 125ms/step - loss: 0.0610 - iou_score: 0.9392 - f1-score: 0.9676 - precision: 0.9678 - val_loss: 0.0559 - val_iou_score: 0.9443 - val_f1-score: 0.9702 - val_precision: 0.9727\n",
      "Epoch 4/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0551 - iou_score: 0.9451 - f1-score: 0.9711 - precision: 0.9709\n",
      "Epoch 4: loss improved from 0.06101 to 0.05506, saving model to resnet34_Linknet.hdf5\n",
      "17415/17415 [==============================] - 2182s 125ms/step - loss: 0.0551 - iou_score: 0.9451 - f1-score: 0.9711 - precision: 0.9709 - val_loss: 0.0669 - val_iou_score: 0.9333 - val_f1-score: 0.9635 - val_precision: 0.9878\n",
      "Epoch 5/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0526 - iou_score: 0.9476 - f1-score: 0.9724 - precision: 0.9726\n",
      "Epoch 5: loss improved from 0.05506 to 0.05255, saving model to resnet34_Linknet.hdf5\n",
      "17415/17415 [==============================] - 2182s 125ms/step - loss: 0.0526 - iou_score: 0.9476 - f1-score: 0.9724 - precision: 0.9726 - val_loss: 0.0735 - val_iou_score: 0.9266 - val_f1-score: 0.9569 - val_precision: 0.9835\n",
      "\n",
      " Total time: 181.0 minutes and 55.79 seconds\n",
      "Found 34830 images belonging to 1 classes.\n",
      "Found 34830 images belonging to 1 classes.\n",
      "Epoch 1/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0880 - iou_score: 0.9138 - f1-score: 0.9519 - precision: 0.9541Found 8932 images belonging to 1 classes.\n",
      "Found 8932 images belonging to 1 classes.\n",
      "\n",
      "Epoch 1: loss improved from inf to 0.08796, saving model to vgg16_Unet.hdf5\n",
      "17415/17415 [==============================] - 3115s 179ms/step - loss: 0.0880 - iou_score: 0.9138 - f1-score: 0.9519 - precision: 0.9541 - val_loss: 0.0453 - val_iou_score: 0.9550 - val_f1-score: 0.9766 - val_precision: 0.9819\n",
      "Epoch 2/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0499 - iou_score: 0.9503 - f1-score: 0.9740 - precision: 0.9748\n",
      "Epoch 2: loss improved from 0.08796 to 0.04991, saving model to vgg16_Unet.hdf5\n",
      "17415/17415 [==============================] - 3105s 178ms/step - loss: 0.0499 - iou_score: 0.9503 - f1-score: 0.9740 - precision: 0.9748 - val_loss: 0.0352 - val_iou_score: 0.9649 - val_f1-score: 0.9820 - val_precision: 0.9826\n",
      "Epoch 3/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0452 - iou_score: 0.9549 - f1-score: 0.9765 - precision: 0.9769\n",
      "Epoch 3: loss improved from 0.04991 to 0.04525, saving model to vgg16_Unet.hdf5\n",
      "17415/17415 [==============================] - 3106s 178ms/step - loss: 0.0452 - iou_score: 0.9549 - f1-score: 0.9765 - precision: 0.9769 - val_loss: 0.0356 - val_iou_score: 0.9645 - val_f1-score: 0.9817 - val_precision: 0.9894\n",
      "Epoch 4/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0415 - iou_score: 0.9585 - f1-score: 0.9785 - precision: 0.9786\n",
      "Epoch 4: loss improved from 0.04525 to 0.04153, saving model to vgg16_Unet.hdf5\n",
      "17415/17415 [==============================] - 3106s 178ms/step - loss: 0.0415 - iou_score: 0.9585 - f1-score: 0.9785 - precision: 0.9786 - val_loss: 0.0334 - val_iou_score: 0.9667 - val_f1-score: 0.9829 - val_precision: 0.9901\n",
      "Epoch 5/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0398 - iou_score: 0.9603 - f1-score: 0.9795 - precision: 0.9794\n",
      "Epoch 5: loss improved from 0.04153 to 0.03978, saving model to vgg16_Unet.hdf5\n",
      "17415/17415 [==============================] - 3108s 178ms/step - loss: 0.0398 - iou_score: 0.9603 - f1-score: 0.9795 - precision: 0.9794 - val_loss: 0.0314 - val_iou_score: 0.9686 - val_f1-score: 0.9839 - val_precision: 0.9904\n",
      "\n",
      " Total time: 259.0 minutes and 1.23 seconds\n",
      "Found 34830 images belonging to 1 classes.\n",
      "Found 34830 images belonging to 1 classes.\n",
      "Epoch 1/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.1025 - iou_score: 0.9000 - f1-score: 0.9437 - precision: 0.9465Found 8932 images belonging to 1 classes.\n",
      "Found 8932 images belonging to 1 classes.\n",
      "\n",
      "Epoch 1: loss improved from inf to 0.10249, saving model to vgg16_Linknet.hdf5\n",
      "17415/17415 [==============================] - 2969s 170ms/step - loss: 0.1025 - iou_score: 0.9000 - f1-score: 0.9437 - precision: 0.9465 - val_loss: 0.0530 - val_iou_score: 0.9475 - val_f1-score: 0.9726 - val_precision: 0.9787\n",
      "Epoch 2/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0595 - iou_score: 0.9409 - f1-score: 0.9689 - precision: 0.9705\n",
      "Epoch 2: loss improved from 0.10249 to 0.05951, saving model to vgg16_Linknet.hdf5\n",
      "17415/17415 [==============================] - 2967s 170ms/step - loss: 0.0595 - iou_score: 0.9409 - f1-score: 0.9689 - precision: 0.9705 - val_loss: 0.0466 - val_iou_score: 0.9539 - val_f1-score: 0.9760 - val_precision: 0.9719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "17415/17415 [==============================] - ETA: 0s - loss: 0.0523 - iou_score: 0.9480 - f1-score: 0.9728 - precision: 0.9736\n",
      "Epoch 3: loss improved from 0.05951 to 0.05229, saving model to vgg16_Linknet.hdf5\n",
      "17415/17415 [==============================] - 2969s 170ms/step - loss: 0.0523 - iou_score: 0.9480 - f1-score: 0.9728 - precision: 0.9736 - val_loss: 0.0440 - val_iou_score: 0.9562 - val_f1-score: 0.9773 - val_precision: 0.9892\n",
      "Epoch 4/5\n",
      "15739/17415 [==========================>...] - ETA: 4:09 - loss: 0.0486 - iou_score: 0.9516 - f1-score: 0.9748 - precision: 0.9753"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m\n\u001b[1;32m     33\u001b[0m csv_logger \u001b[38;5;241m=\u001b[39m CSVLogger(BACKBONE \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DECODER \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(BACKBONE \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DECODER \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     36\u001b[0m                                    monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m                                    verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     38\u001b[0m                                    save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 40\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainGene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtestGene\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps_p_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnr_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m time_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m since\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Total time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(time_elapsed \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(time_elapsed \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index in range(len(all_names_models)):\n",
    "    since = time.time()\n",
    "    BACKBONE = all_names_models[index].split('_')[-1]\n",
    "    DECODER = all_names_models[index].split('_')[-2]\n",
    "    \n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "    trainGene = train_generator(batch_size, \n",
    "                               path_dataset, \n",
    "                               'train',\n",
    "                               'mask_train',\n",
    "                               data_gen_args,\n",
    "                               save_to_dir = None, \n",
    "                               image_color_mode = \"rgb\"\n",
    "                              )\n",
    "\n",
    "    testGene = test_generator(batch_size, \n",
    "                              path_dataset,\n",
    "                              'validation', \n",
    "                              'mask_validation', \n",
    "                              save_to_dir = None,\n",
    "                              image_color_mode = \"rgb\")\n",
    "    \n",
    "    model = all_models[index]\n",
    "    opt = Adam(learning_rate=1e-3)\n",
    "\n",
    "    model.compile(opt,\n",
    "                  loss=sm.losses.jaccard_loss,\n",
    "                  metrics=[sm.metrics.IOUScore(threshold=0.5),\n",
    "                  sm.metrics.FScore(threshold=0.5), \n",
    "                  sm.metrics.precision])\n",
    "\n",
    "    csv_logger = CSVLogger(BACKBONE + \"_\" + DECODER + \".csv\", append=True)\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(BACKBONE + \"_\" + DECODER + \".hdf5\", \n",
    "                                       monitor='loss',\n",
    "                                       verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    \n",
    "    history = model.fit(trainGene, \n",
    "                        validation_data = testGene,\n",
    "                        validation_steps = val_steps,\n",
    "                        steps_per_epoch = steps_p_epoch,\n",
    "                        epochs=nr_epochs,\n",
    "                        callbacks=[model_checkpoint, csv_logger])\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        f\"\\n Total time: {round(time_elapsed // 60, 2)} minutes and {round(time_elapsed % 60, 2)} seconds\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "def create_default_dataset(path_to_folder: str, path_to_save: str, percent_of_images: list = [0.98, 0.02, 0.0]):\n",
    "    \"\"\"Create a dataset with the structure: train, test, validation. \"\"\"\n",
    "    train_percent = percent_of_images[0]\n",
    "    test_percet = percent_of_images[1]\n",
    "    val_percent = percent_of_images[2]\n",
    "\n",
    "    splitfolders.ratio(f\"{path_to_folder}\",\n",
    "                   output=f\"{path_to_save}\",\n",
    "                   seed=42,\n",
    "                   ratio=(train_percent, val_percent, test_percet),\n",
    "                   group_prefix=True,\n",
    "                   move=False\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 14424 files [00:23, 603.70 files/s] \n"
     ]
    }
   ],
   "source": [
    "create_default_dataset(path_to_folder='../dirty_insulators/', path_to_save='new_data/')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "supervisioned_segmentation_vgg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
